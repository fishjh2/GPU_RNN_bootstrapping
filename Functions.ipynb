{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pexpect\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_bootstrap_batch_sorter(features_data, targets_data, min_batch_size, num_steps, num_networks = 100, test_ratio = 0.15,\n",
    "                     val_ratio = 0.15, silent = False, method = 'simple_block'):\n",
    "    \"\"\"\n",
    "    Sorts RNN data properly (i.e. maximises the number of states which are passed on correctly). Will split into\n",
    "    train/val and test data first, and then put the data in iterators to pass to the RNN\n",
    "    \n",
    "    features_data: shape [size_data, num_features]\n",
    "    targets_data: shape [size_data, num_targets]\n",
    "    min_batch_size: minimum batch_size for training data (for test batch_size will be one)\n",
    "    num_steps: number of steps RNN rolled out for during training\n",
    "    num_networks: the number of networks being trained at once\n",
    "    test_ratio: proportion of data for test set. Val proportion will be determined by bootstrap draws.\n",
    "    val_ratio: proportion of the train/val dataset used for val data (only applies if method = simple_block)\n",
    "    silent: whether or not to print batch_sizes \n",
    "    method: the method of bootstrapping. One of ['simple_block']\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary with iterators holding datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    assert targets_data.shape[0] == features_data.shape[0], 'Targets and Features data different sizes'\n",
    "    assert len(targets_data.shape) == 2, 'Targets data needs to be at least 2D'\n",
    "    \n",
    "    num_data_points = features_data.shape[0]\n",
    "    num_features = features_data.shape[1]\n",
    "    num_targets = targets_data.shape[1]\n",
    "        \n",
    "    # Split the data into train/val and test sets\n",
    "    test_size = int(math.ceil(test_ratio * num_data_points))\n",
    "    train_size = int(num_data_points - test_size)\n",
    "    \n",
    "    train_f_data = features_data[0: train_size, :]\n",
    "    train_t_data = targets_data[0: train_size, :]\n",
    "\n",
    "    test_t_data = targets_data[train_size:, :]\n",
    "    test_f_data = features_data[train_size:, :]\n",
    "    \n",
    "    if not silent:\n",
    "        print \"Train/ Val data: {} observations\".format(train_t_data.shape[0])\n",
    "        print \"Test data: {} observations\\n\".format(test_t_data.shape[0])\n",
    "\n",
    "    iter_dict = bootstrap_rnn_data(train_f_data, train_t_data, test_f_data, test_t_data, min_batch_size = min_batch_size,\n",
    "                                  num_steps = num_steps, num_networks = num_networks, val_ratio = val_ratio, silent = silent,\n",
    "                                  method = method)\n",
    "    return iter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_rnn_data(tr_features_data, tr_targets_data, test_features_data, test_targets_data, min_batch_size, \n",
    "                       num_steps, num_networks = 100, val_ratio = 0.15, silent = False, method = 'simple_block'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Bootstraps RNN data properly (i.e. maximises the number of states which are passed on correctly). \n",
    "    Train/Test split already provided. Returns the data in iterators to pass to the RNN\n",
    "    \n",
    "    tr_features_data: shape [size_train_data, num_features], includes both val and train data\n",
    "    tr_targets_data: shape [size_train_data, num_targets], includes both val and train data\n",
    "    test_features_data: shape [size_test_data, num_features]\n",
    "    test_targets_data: shape [size_test_data, num_targets]\n",
    "    min_batch_size: minimum batch_size for training data (for test batch_size will be one)\n",
    "    num_steps: number of steps RNN rolled out for during training\n",
    "    num_networks: the number of networks being trained at once\n",
    "    val_ratio: proportion of the train/val dataset used for val data (only applies if method = simple_block)\n",
    "    silent: whether or not to print batch_sizes \n",
    "    method: the method of bootstrapping. One of ['simple_block']\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary with iterators holding datasets\n",
    "    \"\"\"\n",
    "    train_size = tr_targets_data.shape[0]\n",
    "    \n",
    "    # Make the masks for each network's draw\n",
    "    val_size = int(val_ratio * train_size)\n",
    "    if method == 'simple_block':\n",
    "        mask_list = []\n",
    "        inf_mask_list = []  # A flattened version of the mask list for calculating the val/train losses separately\n",
    "        for b in range(num_networks):\n",
    "            val_start_index = np.random.choice(np.arange(train_size - val_size))\n",
    "            mask = np.ones([tr_targets_data.shape[0], 1])\n",
    "            mask[val_start_index:val_start_index + val_size, :] = 0\n",
    "            mask_batches, _ = sort_batches(mask, min_batch_size = min_batch_size, num_steps = num_steps, \n",
    "                                           dtype = 'train', t_or_f = None)\n",
    "            inf_mask_batches, _ = sort_batches(mask, min_batch_size = min_batch_size, num_steps = num_steps,\n",
    "                                               dtype = 'test', t_or_f = None)\n",
    "            mask_list.append(mask_batches)\n",
    "            inf_mask_list.append(inf_mask_batches)\n",
    "        mask_list = zip(*mask_list)\n",
    "        inf_mask_list = zip(*inf_mask_list)\n",
    "        \n",
    "        all_mask_list = []\n",
    "        for m in mask_list:\n",
    "            all_mask_list.append(np.concatenate([np.expand_dims(i, 0) for i in m]))\n",
    "            \n",
    "        all_inf_mask_list = []\n",
    "        for m in inf_mask_list:\n",
    "            all_inf_mask_list.append(np.concatenate([np.expand_dims(i,0) for i in m]))\n",
    "    \n",
    "    # Sort the data into batches\n",
    "    train_f_batches, tr_seq_lengths = sort_batches(tr_features_data, min_batch_size = min_batch_size, num_steps = num_steps, \n",
    "                                                   dtype = 'train', t_or_f = 'features', silent = silent)\n",
    "    train_t_batches, _  = sort_batches(tr_targets_data, min_batch_size = min_batch_size, num_steps = num_steps,\n",
    "                                       dtype = 'train', t_or_f = 'targets', silent = silent)                \n",
    "    test_f_batches, test_seq_lengths = sort_batches(test_features_data, min_batch_size = min_batch_size, num_steps = num_steps,\n",
    "                                                    dtype = 'test', t_or_f = 'features', silent = silent)\n",
    "    test_t_batches, _  = sort_batches(test_targets_data, min_batch_size = min_batch_size, num_steps = num_steps,\n",
    "                                      dtype = 'test', t_or_f = 'targets', silent = silent)\n",
    "    \n",
    "    # Train data with batch_size of 1 for inference - ensures correct state always passed on\n",
    "    trinf_f_batches, trinf_seq_lengths = sort_batches(tr_features_data, min_batch_size = min_batch_size, num_steps = num_steps,\n",
    "                                                      dtype = 'train_inference', t_or_f = 'features', silent = silent)\n",
    "    trinf_t_batches, _  = sort_batches(tr_targets_data, min_batch_size = min_batch_size, num_steps = num_steps,\n",
    "                                       dtype = 'train_inference', t_or_f = 'targets', silent = silent)\n",
    "    \n",
    "    # Make masks of all ones for the test and train_inf data\n",
    "    test_masks = [np.ones([num_networks, 1, num_steps, 1]) for t in test_t_batches]\n",
    "    trinf_masks = [np.ones([num_networks, 1, num_steps, 1]) for t in trinf_t_batches]\n",
    "\n",
    "    # Put the data in interators\n",
    "    # Iterator for training\n",
    "    train_iter = rnn_batch_iterator(train_f_batches, train_t_batches, tr_seq_lengths, masks = all_mask_list)\n",
    "    \n",
    "    # Iterators for prediction once training finished\n",
    "    test_iter = rnn_batch_iterator(test_f_batches, test_t_batches, test_seq_lengths, masks = test_masks)\n",
    "    trinf_iter = rnn_batch_iterator(trinf_f_batches, trinf_t_batches, trinf_seq_lengths, masks = trinf_masks)\n",
    "        \n",
    "    # Iterator for evaluation of train/val loss during training\n",
    "    tr_loss_iter = rnn_batch_iterator(trinf_f_batches, trinf_t_batches, trinf_seq_lengths, masks = all_inf_mask_list)\n",
    "    \n",
    "    return {'train': train_iter, 'test': test_iter, 'tr_inf': trinf_iter, 'tr_loss': tr_loss_iter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_batches(data, min_batch_size, num_steps, dtype, t_or_f, silent = True):\n",
    "    \"\"\"\n",
    "    Sorts batches in most efficient way for RNN\n",
    "    data: shape [num_data_points, targets/features dimension]\n",
    "    min_batch_size: minimum batch_size for training data (for test batch_size will be one)\n",
    "    num_steps: number of steps RNN rolled out for during training\n",
    "    dtype: one of [train, test, 'train_inference']\n",
    "    t_or_f: one of [features, targets]\n",
    "\n",
    "    Returns:\n",
    "    batches: the data sorted into batches\n",
    "    seq_lengths: the corresponding sequence lengths \n",
    "    \"\"\"\n",
    "\n",
    "    num_data_points = data.shape[0]\n",
    "\n",
    "    if dtype == 'train':\n",
    "        # Calculate the batch size - basically keeping the same number of batches as would be required by min_batch_size,\n",
    "        # but increasing the size to avoid wasting data\n",
    "        batches_required = int(math.floor(num_data_points/float(min_batch_size * num_steps)))\n",
    "        b_length = int(math.floor(num_data_points/float(batches_required * num_steps)))\n",
    "        num_zeros = 0\n",
    "    elif dtype in ['test', 'train_inference']:\n",
    "        b_length = 1\n",
    "\n",
    "        if num_data_points % (num_steps*b_length) == 0:\n",
    "            num_zeros = 0\n",
    "        else:\n",
    "            num_zeros = int((num_steps*b_length) - (num_data_points % (num_steps*b_length)))             \n",
    "\n",
    "    # Use NaN's at this point, so no mistakes if any of features or targets are actually 0\n",
    "    filler = np.empty(shape = [num_zeros, data.shape[1]])\n",
    "    filler[:] = np.NAN\n",
    "    data = np.concatenate([data, filler], axis = 0)\n",
    "    num_batches = int(len(data)/(b_length*num_steps))\n",
    "\n",
    "    batches = []\n",
    "    seq_lengths = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        input_matrix = np.zeros((b_length, num_steps, data.shape[1]))\n",
    "        for j in range(b_length):\n",
    "            row_start_index = (0 + (num_steps*i)) + (j*num_batches*num_steps)\n",
    "            input_matrix[j] = data[row_start_index:(row_start_index + num_steps)]\n",
    "\n",
    "        # Find where the nan's are to calculate sequence length\n",
    "        seq_len = np.ones([b_length]) * num_steps\n",
    "        seq_len[-1] = num_steps - np.sum(np.isnan(input_matrix[-1,:,0]))\n",
    "        seq_lengths.append(seq_len)\n",
    "\n",
    "        # Replace nan's with zeros\n",
    "        input_matrix[np.isnan(input_matrix)] = 0\n",
    "        batches.append(input_matrix)\n",
    "\n",
    "    if silent == False:\n",
    "        print \"{} {} {} batches created, of shape {} [batch_length * num_steps * num_features]\".format(len(batches),\n",
    "                                                                            dtype, t_or_f, batches[0].shape)\n",
    "\n",
    "    return batches, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    def __init__(self, sess, batch_iterators, num_steps, num_layers = 1, hidden_size = 10, cell_type = 'basic', \n",
    "                 activation_fn = tf.nn.tanh, learning_rate = 0.01, pass_state = False, model_name = 'rnn', \n",
    "                 checkpoint_dir = 'rnn_checkpoint'):\n",
    "        \n",
    "        self.sess = sess # A tensorflow session\n",
    "        \n",
    "        # Data iterators - hold the data sets, and return the next batch when next_batch() called\n",
    "        self.train_iter = batch_iterators['train'] # Training data\n",
    "        self.val_iter = batch_iterators['val'] # Val data - batch_length of 1\n",
    "        self.test_iter = batch_iterators['test'] # Test data - batch_length of 1\n",
    "        self.train_inf_iter = batch_iterators['tr_inf'] # Train data for predictions at end - batch_length of 1\n",
    "        \n",
    "        self.num_steps = num_steps # how many steps to unroll the RNN during training\n",
    "        self.num_layers = num_layers # number of layers of the RNN\n",
    "        self.hidden_size = hidden_size # the number of units in each RNN cell\n",
    "        self.cell_type = cell_type # one of ['basic', 'gru', 'lstm'] - which RNN cell to use\n",
    "        self.activation_fn = activation_fn # activation function for the output layer\n",
    "        \n",
    "        self.learning_rate = learning_rate # the learning rate for training\n",
    "        self.pass_state = pass_state # whether to pass state on from last batch in epoch to first batch during training\n",
    "                \n",
    "        self.model_name = model_name # Name for the model - used in name of saved checkpoint files\n",
    "        self.checkpoint_dir = checkpoint_dir # Directory in which to save checkpoint files\n",
    "        \n",
    "        self.targets_dim = self.train_iter.targets_dim # dimension of the target variable\n",
    "        self.features_dim = self.train_iter.features_dim # number of features \n",
    "                \n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        self.features_pl = tf.placeholder(tf.float32, [None, self.num_steps, self.features_dim], 'features_pl')\n",
    "        self.targets_pl = tf.placeholder(tf.float32, [None, self.num_steps, self.targets_dim], 'targets_pl')\n",
    "        self.seq_len_pl = tf.placeholder(tf.int32, shape = [None], name = 'seq_len_pl')\n",
    "        self.b_size_pl = tf.placeholder(tf.int32, shape = (), name = 'b_size_pl')\n",
    "        \n",
    "        # Reshape targets to correct size and crop off any zero padding\n",
    "        targets = tf.concat(tf.unstack(self.targets_pl, num = self.num_steps, axis = 1), axis = 0)\n",
    "        self.targets = targets[:self.b_size_pl, :]\n",
    "        \n",
    "        # Unstack features into num_step length list of [batch_size * features_dim] tensors\n",
    "        self.features = tf.unstack(self.features_pl, num = self.num_steps, axis = 1)\n",
    "        \n",
    "        # Set up the RNN cell\n",
    "        if self.cell_type == 'basic':\n",
    "            cell = tf.contrib.rnn.BasicRNNCell(self.hidden_size, activation = self.activation_fn)\n",
    "        elif self.cell_type == 'gru':\n",
    "            cell = tf.contrib.rnn.GRUCell(self.hidden_size, activation = self.activation_fn)\n",
    "        \n",
    "        multi_cell = tf.contrib.rnn.MultiRNNCell([cell]*self.num_layers, state_is_tuple = True)\n",
    "        \n",
    "        # Placeholders for the states each layer of cells\n",
    "        self.initial_state_list = []\n",
    "        for l in range(self.num_layers):\n",
    "            self.initial_state_list.append(tf.placeholder(tf.float32, [None, self.hidden_size], name = 'init_state_pl_'+ str(l)))\n",
    "        \n",
    "        # Set up the RNN\n",
    "        self.cell_outputs, self.final_state  = tf.contrib.rnn.static_rnn(multi_cell, self.features, \n",
    "                                    initial_state = tuple(self.initial_state_list), sequence_length = self.seq_len_pl)\n",
    "        \n",
    "        # Join outputs into shape [b_length * num_steps, hidden_dim] and pass through output layer\n",
    "        joined_outputs = tf.concat(self.cell_outputs, axis = 0)[:self.b_size_pl, :]\n",
    "        self.output = tf.contrib.layers.fully_connected(joined_outputs, self.targets_dim, activation_fn=None, scope = 'final_layer')\n",
    "        \n",
    "        # Loss        \n",
    "        self.loss = tf.reduce_mean(tf.pow(self.targets - self.output, 2))\n",
    "                  \n",
    "        # Optimizer\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "                \n",
    "    def train(self, viz_every = 500, num_steps = 5000):\n",
    "        \"\"\"\n",
    "        Train the network, and calculates final predictions and loss for each dataset once done\n",
    "        \"\"\"\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        for step in xrange(num_steps):\n",
    "            \n",
    "            if step == 0:\n",
    "                state = self.fill_zero_state(self.train_iter)\n",
    "            if self.train_iter.counter == 0:\n",
    "                if self.pass_state:\n",
    "                    state = self.update_train_state(state)                    \n",
    "                else:\n",
    "                    state = self.fill_zero_state(self.train_iter)\n",
    "                \n",
    "            f_batch, t_batch, seq_len = self.train_iter.next_batch()\n",
    "            b_size = int(seq_len[-1] * f_batch.shape[0])\n",
    "             \n",
    "            # Fill the feed_dict    \n",
    "            feed_dict = {self.features_pl: f_batch, self.targets_pl: t_batch, self.seq_len_pl: seq_len, \n",
    "                         self.b_size_pl: b_size}\n",
    "            for l in range(self.num_layers):\n",
    "                feed_dict[self.initial_state_list[l]] = state[l]\n",
    "            \n",
    "            # Run a train step\n",
    "            ops = {\"opt\": self.opt, \"final_state\": self.final_state}            \n",
    "            returns = self.sess.run(ops, feed_dict = feed_dict)\n",
    "            \n",
    "            # Pass final state of previous batch onto next batch\n",
    "            state = returns[\"final_state\"]\n",
    "\n",
    "            # Check progress\n",
    "            if step % viz_every == 0:\n",
    "                _, TRAIN_LOSS, tr_state= self.run_data_set(self.train_inf_iter)\n",
    "                _, VAL_LOSS, _ = self.run_data_set(self.val_iter, previous_state = tr_state)\n",
    "\n",
    "                print \"Step: {0}, Train Loss: {1:.4f}, Val Loss: {2:.4f}\".format(step,TRAIN_LOSS, VAL_LOSS)            \n",
    "\n",
    "                if VAL_LOSS < best_val_loss:\n",
    "                    self.save()\n",
    "                    best_val_loss = VAL_LOSS\n",
    "                    \n",
    "        # Restore the variables which achieved the lowest validation loss           \n",
    "        self.saver.restore(self.sess, self.checkpoint_dir + '/' + self.model_name)\n",
    "        \n",
    "        # Get final predictions and loss for all data sets\n",
    "        self.TRAIN_PREDS, TRAIN_LOSS, tr_state = self.run_data_set(self.train_inf_iter)\n",
    "        self.VAL_PREDS, VAL_LOSS, val_state = self.run_data_set(self.val_iter, previous_state = tr_state)\n",
    "        self.TEST_PREDS, TEST_LOSS, _ = self.run_data_set(self.test_iter, previous_state = val_state)\n",
    "                \n",
    "        print \"Final Losses, Train: {1:.4f}, Val: {2:.4f}, Test: {3:.4f}\".format(step,\n",
    "                                                                            TRAIN_LOSS, VAL_LOSS, TEST_LOSS) \n",
    "                \n",
    "    def run_data_set(self, iterator, previous_state = None):\n",
    "        \"\"\"\n",
    "        Calculates the predictions and average loss for the whole dataset stored in iterator\n",
    "        \"\"\"\n",
    "        \n",
    "        if previous_state == None:\n",
    "            state = self.fill_zero_state(iterator)\n",
    "        else:\n",
    "            state = previous_state\n",
    "\n",
    "        # Store starting value of iterator to return to\n",
    "        counter_start = iterator.counter\n",
    "        # Make sure we start from the first batch\n",
    "        iterator.counter = 0\n",
    "        \n",
    "        # Lists for storing the returns from each batch\n",
    "        preds_list = []\n",
    "        loss_list = []\n",
    "        \n",
    "        for step in xrange(iterator.num_batches):\n",
    "            \n",
    "            # Get the next batches of data\n",
    "            f_batch, t_batch, seq_len = iterator.next_batch()\n",
    "            b_size = int(seq_len[-1] * f_batch.shape[0])\n",
    "             \n",
    "            # Fill the feed dict\n",
    "            feed_dict = {self.features_pl: f_batch, self.targets_pl: t_batch, \n",
    "                         self.seq_len_pl:seq_len, self.b_size_pl: b_size}\n",
    "            for l in range(self.num_layers):\n",
    "                feed_dict[self.initial_state_list[l]] = state[l]\n",
    "             \n",
    "            # Run the ops\n",
    "            ops = {\"final_state\": self.final_state, \"loss\": self.loss, \"preds\": self.output}          \n",
    "            returns = self.sess.run(ops, feed_dict = feed_dict)\n",
    "            \n",
    "            # Pass final state onto next batch\n",
    "            state = returns[\"final_state\"]\n",
    "            \n",
    "            # Store the loss and predictions from current batch\n",
    "            preds_list.append(returns[\"preds\"])\n",
    "            loss_list.append(returns[\"loss\"])\n",
    "        \n",
    "        # Join the losses and predictions from all the batches\n",
    "        loss = np.average(loss_list)\n",
    "        preds = np.concatenate(preds_list, axis = 0)\n",
    "\n",
    "        # Return iterator counter to starting value\n",
    "        iterator.counter = counter_start\n",
    "        \n",
    "        return preds, loss, state\n",
    "   \n",
    "    def fill_zero_state(self, iter_):  \n",
    "        \"\"\"\n",
    "        Returns state filled with zeros for start of training/eval\n",
    "        iter_: data iterator\n",
    "        \"\"\"\n",
    "        state = []\n",
    "        for l in range(self.num_layers):\n",
    "            state.append(np.zeros([iter_.batch_size, self.hidden_size]))\n",
    "        return state\n",
    "\n",
    "    def update_train_state(self, prev_state):\n",
    "        \"\"\"\n",
    "        Takes the state from last training batch in epoch and shifts it down by one row, so \n",
    "        that correct state is passed to the first training batch in the next epoch\n",
    "        prev_state: the state from the final batch of the previous epoch\n",
    "        \"\"\"\n",
    "        old_state = list(prev_state)\n",
    "        new_state = []\n",
    "        for l in range(self.num_layers):\n",
    "            s = old_state[l]\n",
    "            new_s = np.concatenate([np.zeros([1, self.hidden_size], \n",
    "                                dtype = np.float64), s], axis = 0)[:-1,:]\n",
    "            new_state.append(new_s)        \n",
    "        return tuple(new_state)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves a checkpoint file\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        self.saver.save(self.sess, self.checkpoint_dir + '/' + self.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(targets, predictions):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error\n",
    "    \"\"\"\n",
    "    assert targets.shape == predictions.shape, 'Targets and predictions arrays not the same shape'\n",
    "    residuals = targets - predictions\n",
    "    residuals_squared = residuals ** 2\n",
    "    MSE = np.average(residuals_squared)\n",
    "    return MSE   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lin_scale(data, low = 0.0, high = 1.0):\n",
    "    \"\"\"\n",
    "    Scales data between low and high\n",
    "    \"\"\"\n",
    "    assert len(data.shape) == 2, \"Data needs to be 2d\"\n",
    "    ratio = (high-low)/(np.max(data, axis = 0) - np.min(data, axis = 0))\n",
    "    return low + (ratio * (data-np.min(data, axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rnn_batch_iterator(object):\n",
    "    \"\"\"\n",
    "    RNN batch iterator - holds the data, and returns the next batch of features, targets, seq_lengths when \n",
    "    next_batch() is called\n",
    "    masks: option to return masks as well when next_batch() called - used for masking out val data during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_data, targets_data, seq_lengths, masks = None):\n",
    "\n",
    "        self.features_data = features_data\n",
    "        self.targets_data = targets_data\n",
    "        self.seq_lengths = seq_lengths\n",
    "        if masks:\n",
    "            self.masks = masks\n",
    "        else:\n",
    "            self.masks = None\n",
    "                \n",
    "        self.counter = 0\n",
    "        self.num_batches = len(targets_data)\n",
    "        \n",
    "        self.batch_size = targets_data[0].shape[0]\n",
    "        self.targets_dim = targets_data[0].shape[2]\n",
    "        self.features_dim = features_data[0].shape[2]\n",
    "\n",
    "    def next_batch(self):\n",
    "        \n",
    "        features_batch = self.features_data[self.counter]\n",
    "        targets_batch = self.targets_data[self.counter]\n",
    "        seq_length = self.seq_lengths[self.counter]\n",
    "        if self.masks:\n",
    "            mask_batch = self.masks[self.counter]\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "        if self.counter == self.num_batches:\n",
    "            self.counter = 0\n",
    "        \n",
    "        if self.masks:\n",
    "            return features_batch, targets_batch, seq_length, mask_batch\n",
    "        else:\n",
    "            return features_batch, targets_batch, seq_length\n",
    "        \n",
    "    def all_targets(self):\n",
    "        return np.squeeze(np.concatenate(self.targets_data, axis = 1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_rnn_data(features_data, targets_data, data_set, min_batch_size, num_steps, silent = False):\n",
    "    \"\"\"\n",
    "    Sorts RNN data properly (i.e. maximises the number of states which are passed on correctly)\n",
    "    \n",
    "    features_data: shape [size_data * num_features]\n",
    "    targets_data: shape [size_data * num_targets]\n",
    "    data_set: one of ['train', 'val', 'test', 'train_inference']\n",
    "    train_batch_size: batch_size for training data (for val and test batch_size will be one)\n",
    "    num_steps: number of steps RNN rolled out for during training\n",
    "    \"\"\"\n",
    "    \n",
    "    assert targets_data.shape[0] == features_data.shape[0], 'Targets and Features data different sizes'\n",
    "    assert len(targets_data.shape) == 2, 'Targets data needs to be at least 2D'\n",
    "    \n",
    "    num_data_points = features_data.shape[0]\n",
    "    num_features = features_data.shape[1]\n",
    "    num_targets = targets_data.shape[1]\n",
    "        \n",
    "    if data_set == 'train':\n",
    "         # Calculate the batch size - basically keeping the same number of batches as would be required by batch_size,\n",
    "        # but increasing the size to avoid wasting data\n",
    "        batches_required = int(math.floor(num_data_points/float(min_batch_size * num_steps)))\n",
    "        b_length = int(math.floor(num_data_points/float(batches_required * num_steps)))\n",
    "        num_zeros = 0\n",
    "        \n",
    "    if data_set in ['val', 'test', 'train_inference']:\n",
    "        \n",
    "        b_length = 1\n",
    "\n",
    "        if num_data_points % (num_steps*b_length) == 0:\n",
    "            num_zeros = 0\n",
    "        else:\n",
    "            num_zeros = int((num_steps*b_length) - (num_data_points % (num_steps*b_length)))\n",
    "    \n",
    "\n",
    "    def sort_batches(data):\n",
    "        # Use NaN's at this point, so no mistakes if any of features or targets are actually 0\n",
    "        if data_set in ['val', 'test', 'train_inference']:\n",
    "            filler = np.empty(shape = [num_zeros, data.shape[1]])\n",
    "            filler[:] = np.NAN\n",
    "            data = np.concatenate([data, filler], axis = 0)\n",
    "        num_batches = int(len(data)/(b_length*num_steps))\n",
    "        \n",
    "        batches = []\n",
    "        seq_lengths = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            input_matrix = np.zeros((b_length, num_steps, data.shape[1]))\n",
    "            for j in range(b_length):\n",
    "                row_start_index = (0 + (num_steps*i)) + (j*num_batches*num_steps)\n",
    "                input_matrix[j] = data[row_start_index:(row_start_index + num_steps)]\n",
    "            \n",
    "            # Find where the nan's are to calculate sequence length\n",
    "            seq_len = np.ones([b_length]) * num_steps\n",
    "            seq_len[-1] = num_steps - np.sum(np.isnan(input_matrix[-1,:,0]))\n",
    "            seq_lengths.append(seq_len)\n",
    "            \n",
    "            # Replace nan's with zeros\n",
    "            input_matrix[np.isnan(input_matrix)] = 0\n",
    "            batches.append(input_matrix)\n",
    "            \n",
    "            #batch_sizes.append(input_matrix.shape[0] * int(seq_len[-1]))\n",
    "            #batches.append([np.squeeze(i, axis = 1) for i in np.split(input_matrix, num_steps, axis = 1)])\n",
    "        return batches, seq_lengths\n",
    "    \n",
    "    features_batches,  seq_lengths = sort_batches(features_data)\n",
    "    targets_batches, _ = sort_batches(targets_data)\n",
    "        \n",
    "    if silent == False:\n",
    "        print \"{} {} feature batches created, of shape {} [batch_length * num_steps * num_features]\".format(len(features_batches), data_set, \n",
    "                                                                                                features_batches[0].shape)\n",
    "        print \"{} {} target batches created, in of shape {} [batch_length * num_steps * num_targets]\\n\" .format(len(targets_batches), data_set, \n",
    "                                                                                                targets_batches[0].shape) \n",
    "    \n",
    "    return features_batches, targets_batches, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_blog_graph():\n",
    "    fig = plt.figure(figsize = [6,4])\n",
    "    ax = plt.axes()    \n",
    "    mpl.rc('axes', labelsize = 12)\n",
    "    mpl.rc('figure', titlesize = 14)\n",
    "    return fig, ax\n",
    "\n",
    "colours = {'orange': '#f78500', 'yellow': '#fed16c', 'green': '#139277', 'blue': '#0072df',\n",
    "               'dark_blue': '#001e78', 'pink': '#fd6d77'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lag_creator(time_series, num_lags):\n",
    "    \"\"\"\n",
    "    Takes a time series (np array of shape (num_obs,)) and creates lags for it, returning the targets and features\n",
    "    separately as numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(time_series.shape) == 2:\n",
    "        time_series = np.squeeze(time_series)\n",
    "    \n",
    "    features = np.zeros(shape = [len(time_series), num_lags])\n",
    "    \n",
    "    for num,obs in enumerate(time_series):\n",
    "        if (num+1) <= num_lags:\n",
    "            continue\n",
    "        else:\n",
    "            features[num, :] = time_series[num - num_lags:num][::-1]\n",
    "    \n",
    "    features = features[num_lags:,:]\n",
    "    targets = np.expand_dims(time_series[num_lags:], 1)\n",
    "    \n",
    "    return targets, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class monitor_gpu(object):\n",
    "    \"\"\"\n",
    "    class to facilitate monitoring of gpu utilization during training. Call start_monitoring() when training starts\n",
    "    and stop_monitoring() when it has finished.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.command = 'nvidia-smi --query-gpu=utilization.gpu --format=csv -l 1 -f ./temp_gpu_log.csv'\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        self.p = pexpect.spawn(self.command)\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        self.p.sendcontrol('c')\n",
    "        time.sleep(1)\n",
    "        df = pd.read_csv('temp_gpu_log.csv', sep = ' ')\n",
    "        self.usage = df['utilization.gpu'].iloc[2:-1]\n",
    "        self.average_use = np.average(self.usage)\n",
    "        \n",
    "        os.remove('temp_gpu_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_batch_sorter(features_data, targets_data, batch_size = 20, train_ratio = 0.7,\n",
    "                 val_ratio = 0.15, test_ratio = 0.15, num_steps = 5):\n",
    "\n",
    "            \n",
    "    assert train_ratio + val_ratio + test_ratio == 1, 'Percentages don\\'t add up for the data sets'\n",
    "    assert len(targets_data) == len(features_data), 'Targets and features data different sizes'\n",
    "    assert len(targets_data.shape) == 2, 'Targets data needs to be at least 2D'\n",
    "        \n",
    "    # Split the data into train, val and test sets\n",
    "    train_size = int(math.ceil(train_ratio * len(targets_data)))\n",
    "    test_size = int(math.ceil(test_ratio * len(targets_data)))\n",
    "    val_size = int(len(targets_data) - train_size - test_size)\n",
    "    \n",
    "    train_f_data = features_data[0: train_size, :]\n",
    "    train_t_data = targets_data[0: train_size, :]\n",
    "\n",
    "    val_f_data = features_data[train_size: train_size + val_size, :]\n",
    "    val_t_data = targets_data[train_size: train_size + val_size, :]\n",
    "\n",
    "    test_t_data = targets_data[train_size + val_size :, :]\n",
    "    test_f_data = features_data[train_size + val_size :, :]\n",
    "    \n",
    "    print \"Train data: {} observations\".format(train_t_data.shape[0])\n",
    "    print \"Val data: {} observations\".format(val_t_data.shape[0])\n",
    "    print \"Test data: {} observations\\n\".format(test_t_data.shape[0])\n",
    "\n",
    "    # Sort for RNN\n",
    "    train_f, train_t, train_seq_lengths = prepare_rnn_data(train_f_data, train_t_data, data_set = 'train', \n",
    "                                                     min_batch_size = batch_size, num_steps = num_steps)\n",
    "    val_f, val_t, val_seq_lengths = prepare_rnn_data(val_f_data, val_t_data, data_set = 'val', \n",
    "                                                         min_batch_size = batch_size, num_steps = num_steps)\n",
    "    test_f, test_t, test_seq_lengths = prepare_rnn_data(test_f_data, test_t_data, data_set = 'test', \n",
    "                                                         min_batch_size = batch_size, num_steps = num_steps)\n",
    "    trinf_f, trinf_t, trinf_seq_lengths = prepare_rnn_data(train_f_data, train_t_data, data_set = 'train_inference', \n",
    "                                                         min_batch_size = batch_size, num_steps = num_steps)\n",
    "\n",
    "    # Create the iterators\n",
    "    train_iter = rnn_batch_iterator(train_f, train_t, train_seq_lengths)\n",
    "    val_iter = rnn_batch_iterator(val_f, val_t, val_seq_lengths)\n",
    "    test_iter = rnn_batch_iterator(test_f, test_t, test_seq_lengths)\n",
    "    tr_inf_iter = rnn_batch_iterator(trinf_f, trinf_t, trinf_seq_lengths)\n",
    "\n",
    "    iter_dict = {'train': train_iter, 'val': val_iter, 'test': test_iter, 'tr_inf': tr_inf_iter}\n",
    "    \n",
    "    return iter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
